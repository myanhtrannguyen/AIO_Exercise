{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "I, K Nearest Neighbors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee460b0b7d195bee"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ff560f4bacf42327"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Exam 3\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Load the diabetes dataset\n",
    "iris_X, iris_y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "# Split train:test = 8 : 2\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_X,\n",
    "                                                    iris_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state = 42)\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build KNN Classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn_classifier.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b31e96a20da6d778"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Exam 7\n",
    "!pip install -q datasets\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load IMDB dataset\n",
    "imdb = load_dataset(\"imdb\")\n",
    "imdb_train, imdb_test = imdb[\"train\"], imdb[\"test\"]\n",
    "\n",
    "#Convert text to vector using BoW\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(imdb_train[\"text\"]).toarray()\n",
    "X_test = vectorizer.transform(imdb_test[\"text\"]).toarray()\n",
    "\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build KNN Classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_classifier.fit(X_train, imdb_train[\"label\"])\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = knn_classifier.predict(X_test)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b642c18125212eb0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "II, K Means"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42d7cb5323a5d86b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd# Data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e8fde7466c04cad"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = np.array([\n",
    "    [2.0, 3.0, 1.5],\n",
    "    [3.0, 3.5, 2.0],\n",
    "    [3.5, 3.0, 2.5],\n",
    "    [8.0, 8.0, 7.5],\n",
    "    [8.5, 8.5, 8.0],\n",
    "    [9.0, 8.0, 8.5],\n",
    "    [1.0, 2.0, 1.0],\n",
    "    [1.5, 2.5, 1.5],\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cccd6e5a63d3dd4b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=['Feature1', 'Feature2', 'Feature3'])\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8af985f75a9e4aa5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data.shape"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abe2f380174c44ab"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Exam 8\n",
    "centroids = np.array([\n",
    "    [2.0, 3.0, 1.5],\n",
    "    [1.0, 2.0, 1.0],\n",
    "])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecd964b32bc4891f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Exam 9\n",
    "def euclidean_distance(x1, x2):\n",
    "    \"\"\"\n",
    "    Tính khoảng cách Euclid giữa hai điểm dữ liệu\n",
    "    Parameters:\n",
    "        x1 (numpy.ndarray): điểm dữ liệu 1\n",
    "        x2 (numpy.ndarray): điểm dữ liệu 2\n",
    "    Return:\n",
    "        float: khoảng cách Euclid\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.sum(np.power(x1 - x2, 2)))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82092ebbc97e2c69"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "euclidean_distance(np.array([2.0, 3.0, 1.5]), [8.0, 8.0, 7.5])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc82059590567aa1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Exam 10\n",
    "# cụm 1\n",
    "euclidean_distance(np.array([3.0, 3.5, 2.0]), centroids[0])\n",
    "\n",
    "# cụm 2\n",
    "euclidean_distance(np.array([3.0, 3.5, 2.0]), centroids[1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abf6e3ec0a9d82ea"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class KMeans:\n",
    "    def __init__(self, k=3, max_iters=100):\n",
    "        self.k = k                   # Số cụm\n",
    "        self.max_iters = max_iters   # Số vòng lặp tối đa\n",
    "        self.centroids = None        # Tọa độ tâm cụm\n",
    "        self.clusters = None         # Cụm của từng điểm dữ liệu\n",
    "\n",
    "    def initialize_centroids(self, data):\n",
    "        \"\"\"\n",
    "        Khởi tạo ngẫu nhiên tâm cụm\n",
    "        Parameters:\n",
    "            data (numpy.ndarray): dữ liệu đầu vào cần phân cụm\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        self.centroids = data[np.random.choice(data.shape[0], self.k, replace=False)]\n",
    "\n",
    "    def euclidean_distance(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Tính khoảng cách Euclid giữa hai điểm dữ liệu\n",
    "        Parameters:\n",
    "            x1 (numpy.ndarray): điểm dữ liệu 1\n",
    "            x2 (numpy.ndarray): điểm dữ liệu 2\n",
    "        Return:\n",
    "            float: khoảng cách Euclid\n",
    "        \"\"\"\n",
    "        return np.sqrt(np.sum(np.power(x1 - x2, 2)))\n",
    "\n",
    "    def assign_clusters(self, data):\n",
    "        \"\"\"\n",
    "        Phân cụm dữ liệu\n",
    "        Parameters:\n",
    "            data (numpy.ndarray): dữ liệu đầu vào cần phân cụm\n",
    "        Return:\n",
    "            numpy.ndarray: mảng chứa cluster của từng điểm dữ liệu\n",
    "        \"\"\"\n",
    "\n",
    "        # Tính toán khoảng cách giữa mỗi điểm dữ liệu (data point) và tâm (centroids) bằng cách sử dụng hàm euclidean_distance\n",
    "        distances = np.array([[self.euclidean_distance(x, centroid) for centroid in self.centroids] for x in data])\n",
    "\n",
    "        # print(np.argmin(distances, axis=1)) # Có thể in ra dòng này để thấy cách biểu diễn mảng chứa allocation\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "    def update_centroids(self, data):\n",
    "        \"\"\"\n",
    "        Cập nhật tâm cụm\n",
    "        Parameters:\n",
    "            data (numpy.ndarray): dữ liệu đầu vào cần phân cụm\n",
    "        Return:\n",
    "            numpy.ndarray: mảng chứa tâm cụm mới\n",
    "        \"\"\"\n",
    "        return np.array([data[self.clusters == i].mean(axis=0) for i in range(self.k)])\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        Hàm huấn luyện\n",
    "        Parameters:\n",
    "            data (numpy.ndarray): dữ liệu đầu vào cần phân cụm\n",
    "        Return:\n",
    "            None\n",
    "        \"\"\"\n",
    "        # Gọi tới phương thức khởi tạo ngẫu nhiên tâm cụm\n",
    "        self.initialize_centroids(data)\n",
    "\n",
    "        for i in range(self.max_iters):\n",
    "            # Gán cụm cho các data point gần nhất\n",
    "            self.clusters = self.assign_clusters(data)\n",
    "\n",
    "            # Dựa vào các data point của từng cụm, dịch chuyển tâm cụm tới vị trí trung tâm (tính mean) của cụm\n",
    "            new_centroids = self.update_centroids(data)\n",
    "\n",
    "            # Nếu tâm cụm không di chuyển, dừng lại\n",
    "            if np.all(self.centroids == new_centroids):\n",
    "                break\n",
    "\n",
    "            # Nếu tâm cụm có di chuyển, thực hiện lại vòng lặp với các giá trị tâm cụm mới\n",
    "            self.centroids = new_centroids\n",
    "        print(self.centroids)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b25ec02a16f866d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Result\n",
    "kmeans = KMeans(k=3)\n",
    "kmeans.fit(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "894faf5862cdfc8c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
